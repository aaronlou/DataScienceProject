{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手撕决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇文档最大的特点就是把笔者的思考过程写了出来，希望对于初学者如何去组织自己的思路，一步一步实现一个相对复杂的任务有所帮助。毕竟完成授人以鱼，不如授人以渔。  \n",
    "需要提示的是：本文有些细节为了讲述的便利，假定了输入数据中使用了某些特定的变量名称，例如假设目标变量的字段名称为class。这牺牲了一定的通用性。大家可根据自己的需要来进行改造。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/Users/lousiyuan/dataScience/gitSpace/DataScienceProject/Data/watermelon3_0.csv\",header=None)  #这一句要替换为自己电脑上的文件路径\n",
    "data.columns = [\"index\",\"color\",\"root\",\"noise\",\"texture\",\"navel\",\"touch\",\"density\",\"sugar\",\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>color</th>\n",
       "      <th>root</th>\n",
       "      <th>noise</th>\n",
       "      <th>texture</th>\n",
       "      <th>navel</th>\n",
       "      <th>touch</th>\n",
       "      <th>density</th>\n",
       "      <th>sugar</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>青绿</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.264</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index color root noise texture navel touch  density  sugar class\n",
       "0      1    青绿   蜷缩    浊响      清晰    凹陷    硬滑    0.697  0.460     是\n",
       "1      2    乌黑   蜷缩    沉闷      清晰    凹陷    硬滑    0.774  0.376     是\n",
       "2      3    乌黑   蜷缩    浊响      清晰    凹陷    硬滑    0.634  0.264     是"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还记得吗？第一个索引列其实是不需要的，这次我们用一个以前没有用过的操作来舍弃该列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>root</th>\n",
       "      <th>noise</th>\n",
       "      <th>texture</th>\n",
       "      <th>navel</th>\n",
       "      <th>touch</th>\n",
       "      <th>density</th>\n",
       "      <th>sugar</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>青绿</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.264</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  color root noise texture navel touch  density  sugar class\n",
       "0    青绿   蜷缩    浊响      清晰    凹陷    硬滑    0.697  0.460     是\n",
       "1    乌黑   蜷缩    沉闷      清晰    凹陷    硬滑    0.774  0.376     是\n",
       "2    乌黑   蜷缩    浊响      清晰    凹陷    硬滑    0.634  0.264     是"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['index'],axis=1)  #axis=1表示要删除列\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始手动写一个决策树吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "啥？头绪很乱，想不清楚对吧？那就慢慢来呗！ \n",
    "\n",
    "程序编写的重点之一就是把一个大的想法不断拆分成若干小的模块，最后加以整合。这样一个一个小的模块，可以是函数、类、包等不同的形式。其实自定义一个一个函数，合并成一个大的文件，就是最简单的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我在脑海里把决策树的执行过程想了一遍。然后，我意识到，里面其实是一个迭代、反复地分枝的过程。所以，第一步我想要实现一个分枝选择的功能。\n",
    "\n",
    "那么，有哪些可供选择的分枝点呢？我们的数据有很多字段（列），每一个字段就是一个可以拿来进行分枝的节点，但一个字段中会有很多的值，在哪些值上进行分叉呢？这里面就有点门道了。我们知道决策树的核心就是分枝选择，不同的牛人提出了不同的分枝选择依据，所以才会有了ID3、C4.5、CART等不同的策略。这里我们以信息熵方法做为演示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个字段是一个可供选择的分枝对象，里面又有不同的数值可做为分枝的判断条件，这样一个数据的需求，让我们想到了python中的字典(dict）数据结构。每个字段可以做为KEY，字段里可做为分枝选择条件的值用一个列表（list)来保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_possible_branch(data):\n",
    "    possible_branch = defaultdict(list)\n",
    "    #遍历所有字段，得到每个字段可能的分裂点\n",
    "    #注意最后一列是目标变量，不能做为分裂选择的变量\n",
    "    for col in data.drop([\"class\"],axis=1).columns:\n",
    "        #假装我们已经有了一个函数可以用来返回每个字段上的可能值\n",
    "        possible_branch[col] = get_possible_value(data,str(col))   \n",
    "    return possible_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时我们就意识到：哦，还需要一个获得每个字段可能做为分裂条件值的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_value(data,col):\n",
    "    possible_value = []\n",
    "    # 呃，怎么办，好像连续型字段与离散型字段的处理方法不一样唉\n",
    "    return possible_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，单纯依靠数据中呈现的数值本身，并不能准备断定一个字段是连续的还是离散的，比如一个表示年龄的字段，就是一个一个有序数值；一个数值为0，1，2的字段，也有可能只是某一个离散变量。所以我们可以默认把字段当作连续型的，这也是很多决策树算法包的处理方式。其实，这也是为什么我们需要对离散变量进行one-hot-encoding处理的原因所在嘛！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，有些最近的算法包已经可以直接处理类别型变量了，例如catboost，这是一种集成学习算法。感兴趣的同学可以关注本人微信公众号：《实战数据分析挖掘》（funCodingFunLife） ，上面有关于catboost原理、编码的实战讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那连续变量，怎么得到一系列分裂点呢？通常的做法是对变量值先去重，再进行排序，然后每相邻两个值取平均数，生成一个分裂的数值，如此得到一系列值。  \n",
    "So, 我们可以继续写上面的函数了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_possible_value(data,col):\n",
    "    possible_value = []\n",
    "    unique_values = np.unique(data[col])   #numpy.unique本身返回的就已经是排序后的值，所以不再需要单独排序\n",
    "    if len(unique_values) == 2:\n",
    "        potential_split = (unique_values[0] + unique_values[1]) / 2\n",
    "        possible_value.append(potential_split)\n",
    "    else:\n",
    "        for index in range(1,len(unique_values)):   #细心的同学可以思考下这里为什么从下标1开始迭代？\n",
    "            current_value = unique_values[index]\n",
    "            previous_value = unique_values[index - 1]\n",
    "            potential_split = (current_value + previous_value) / 2\n",
    "            possible_value.append(potential_split)\n",
    "    return possible_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们已经写了3个自定义函数了，要及时测试一下是否功能表现如我们所预期。注意这里仅仅是测试，后面还会正式进行数据预处理。  \n",
    "要记得先对类别型变量进行one-hot-encoding处理哦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_乌黑</th>\n",
       "      <th>x0_浅白</th>\n",
       "      <th>x0_青绿</th>\n",
       "      <th>x1_硬挺</th>\n",
       "      <th>x1_稍蜷</th>\n",
       "      <th>x1_蜷缩</th>\n",
       "      <th>x2_沉闷</th>\n",
       "      <th>x2_浊响</th>\n",
       "      <th>x2_清脆</th>\n",
       "      <th>x3_模糊</th>\n",
       "      <th>x3_清晰</th>\n",
       "      <th>x3_稍糊</th>\n",
       "      <th>x4_凹陷</th>\n",
       "      <th>x4_平坦</th>\n",
       "      <th>x4_稍凹</th>\n",
       "      <th>x5_硬滑</th>\n",
       "      <th>x5_软粘</th>\n",
       "      <th>density</th>\n",
       "      <th>sugar</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_乌黑  x0_浅白  x0_青绿  x1_硬挺  x1_稍蜷  x1_蜷缩  x2_沉闷  x2_浊响  x2_清脆  x3_模糊  \\\n",
       "0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0    0.0   \n",
       "1    1.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0    0.0    0.0   \n",
       "\n",
       "   x3_清晰  x3_稍糊  x4_凹陷  x4_平坦  x4_稍凹  x5_硬滑  x5_软粘  density  sugar  class  \n",
       "0    1.0    0.0    1.0    0.0    0.0    1.0    0.0    0.697  0.460      1  \n",
       "1    1.0    0.0    1.0    0.0    0.0    1.0    0.0    0.774  0.376      1  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对特征变量进行 ONE-HOT-ENCODING\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categories=\"auto\",handle_unknown = 'ignore')\n",
    "\n",
    "#对类别型的变量进行 ONE-HOT-ENCODING\n",
    "data_cate = data[[\"color\",\"root\",\"noise\",\"texture\",\"navel\",\"touch\"]]\n",
    "ohe = ohe.fit(data_cate)\n",
    "data_cate_ohe = pd.DataFrame(ohe.transform(data_cate).toarray())\n",
    "data_cate_ohe.columns = ohe.get_feature_names()\n",
    "\n",
    "data_continuous = pd.DataFrame(data[[\"density\",\"sugar\"]]).reset_index(drop=True)  #注意这里对索引进行重新排序，否则后面合并的时候会出现错位\n",
    "\n",
    "data_label = pd.DataFrame(data[\"class\"].map({\"是\":1,\"否\":0})).reset_index(drop=True) \n",
    "\n",
    "#再跟原来的特征变量合并到一起\n",
    "data_concat = pd.concat([data_cate_ohe,data_continuous,data_label],axis=1)\n",
    "data_concat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'x0_乌黑': [0.5],\n",
       "             'x0_浅白': [0.5],\n",
       "             'x0_青绿': [0.5],\n",
       "             'x1_硬挺': [0.5],\n",
       "             'x1_稍蜷': [0.5],\n",
       "             'x1_蜷缩': [0.5],\n",
       "             'x2_沉闷': [0.5],\n",
       "             'x2_浊响': [0.5],\n",
       "             'x2_清脆': [0.5],\n",
       "             'x3_模糊': [0.5],\n",
       "             'x3_清晰': [0.5],\n",
       "             'x3_稍糊': [0.5],\n",
       "             'x4_凹陷': [0.5],\n",
       "             'x4_平坦': [0.5],\n",
       "             'x4_稍凹': [0.5],\n",
       "             'x5_硬滑': [0.5],\n",
       "             'x5_软粘': [0.5],\n",
       "             'density': [0.244,\n",
       "              0.294,\n",
       "              0.3515,\n",
       "              0.38149999999999995,\n",
       "              0.42000000000000004,\n",
       "              0.4590000000000001,\n",
       "              0.5185000000000001,\n",
       "              0.5745,\n",
       "              0.6005,\n",
       "              0.621,\n",
       "              0.6365000000000001,\n",
       "              0.648,\n",
       "              0.6615,\n",
       "              0.6815,\n",
       "              0.7080000000000001,\n",
       "              0.7465],\n",
       "             'sugar': [0.0495,\n",
       "              0.074,\n",
       "              0.095,\n",
       "              0.101,\n",
       "              0.126,\n",
       "              0.155,\n",
       "              0.1795,\n",
       "              0.20450000000000002,\n",
       "              0.21300000000000002,\n",
       "              0.22599999999999998,\n",
       "              0.2505,\n",
       "              0.2655,\n",
       "              0.2925,\n",
       "              0.344,\n",
       "              0.373,\n",
       "              0.41800000000000004]})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_possible_branch(data_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经知道了有哪些可以分裂的节点，每个节点上有哪些候选数值可以作为判断的依据。那么问题来了？怎么选择最优的分裂点？   \n",
    "回想一下，我们上文说本文用信息熵来做为判断依据。所以我们还需要创建一个计算信息熵的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    #取最后一列标签值\n",
    "    label_column = data.iloc[:,-1]\n",
    "    _, counts = np.unique(label_column, return_counts=True) #这个操作高级了！ 一下子同时得到枚举值列表，以及各枚举值的数量； 这里counts其实是一个列表\n",
    "    probabilities = counts / counts.sum()   #这里probabilities仍然是一个列表\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975025463691152"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(data_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了计算熵的函数后，我们就可以基于此来计算分枝前后得到的信息增益，然后横向对比所有可能的分裂点和分裂值，找出信息增益最大的那个做为最终的分裂点和值。  \n",
    "分枝后，原来的数据集被拆成了左枝、右枝，新的信息熵是两个集合的加权和，权重是各自样本量占原来总样本量的比例。  \n",
    "回顾了这些基础后，我们就可以动手来做了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    return overall_entropy\n",
    "\n",
    "\n",
    "#定义一个分枝的函数，根据某个特征变量的某个值，将原数据集划分成左枝一个数据集、右枝一个数据集\n",
    "def split_data(data, split_column, split_value):\n",
    "    split_column_values = data.loc[:, str(split_column)]\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def determine_best_split(data, potential_splits):\n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:    #还记得可行分裂点的格式吗？是个字典，key是特征变量，value是一系列可行的分裂点构成的列表list\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sugar', 0.126)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_splits = get_possible_branch(data_concat)\n",
    "a,b=determine_best_split(data_concat,potential_splits)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c,d = split_data(data_concat,a,b)\n",
    "len(c),len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们在脑海中想一想怎么不断地分枝下去？每一次决定是否要分枝时，其实需要看看当前数据的目标变量是不是“纯”的————只有某一个类了，如果是，那就不需要继续分下去了。所以这里我们需要一个函数来检查数据的纯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    label_column = data.iloc[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_purity(data_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续。  \n",
    "分了一个新枝后，在这个新枝上的样本集，怎么来训练得出其目标变量呢？ 如果已经只剩一个目标变量值了，那显然将该数据集上所有样本均预测为该值即可。但如果还有多个目标值呢？即当前的数据并不“纯”，那就少数服从多数进行投票吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    label_column = data.iloc[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "    index = counts_unique_classes.argmax() #得到频数最大值的下标\n",
    "    classification = unique_classes[index]\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(data, counter=0):\n",
    "    #如果数据已经仅有一个分类了，说明已经是纯的数据了，就不再需要继续分枝\n",
    "    if check_purity(data):   \n",
    "        classification = classify_data(data)\n",
    "        return classification\n",
    "    else:    \n",
    "        counter += 1\n",
    "        # helper functions \n",
    "        potential_splits = get_possible_branch(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        # instantiate sub-tree\n",
    "        question = \"{} <= {}\".format(split_column, split_value)\n",
    "        sub_tree = {question: []}\n",
    "        # find answers (recursion)\n",
    "        yes_answer = decision_tree_algorithm(data_below, counter)\n",
    "        no_answer = decision_tree_algorithm(data_above, counter)\n",
    "        sub_tree[question].append(yes_answer)\n",
    "        sub_tree[question].append(no_answer)\n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sugar <= 0.126': [0,\n",
      "                    {'x1_蜷缩 <= 0.5': [{'x4_稍凹 <= 0.5': [0,\n",
      "                                                        {'sugar <= 0.3035': [1,\n",
      "                                                                             0]}]},\n",
      "                                      1]}]}\n"
     ]
    }
   ],
   "source": [
    "tree = decision_tree_algorithm(data_concat,0)\n",
    "from pprint import pprint\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，一棵决策树模型经过训练后，留下来的是什么呢？就是每一步使用哪个特征变量的哪个值来进行分裂。然后面对未知的数据，根据这些规则来执行就是了。但使用什么样的数据结构来保存这些规则，确实值得好好思考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar <= 0.126'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tree.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split()\n",
    "    # ask question\n",
    "    if example[feature_name] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_example(example, residual_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 来吧，训练+预测+评估来一把"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上文很多自定义函数的逻辑中，估计大家也猜到了。本文中我们拆分数据时，需要将特征变量与目标变量放在一起。所以我们先不用sklearn中自带的train_test_split。而是自己编写一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(df, test_size):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 3)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df , test_df = train_test_split(data,0.2)\n",
    "len(train_df),len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对特征变量进行 ONE-HOT-ENCODING\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categories=\"auto\",handle_unknown = 'ignore')\n",
    "\n",
    "#对类别型的变量进行 ONE-HOT-ENCODING\n",
    "train_cate = train_df[[\"color\",\"root\",\"noise\",\"texture\",\"navel\",\"touch\"]]\n",
    "ohe = ohe.fit(train_cate)\n",
    "train_cate_ohe = pd.DataFrame(ohe.transform(train_cate).toarray())\n",
    "train_cate_ohe.columns = ohe.get_feature_names()\n",
    "\n",
    "train_continuous = pd.DataFrame(train_df[[\"density\",\"sugar\"]]).reset_index(drop=True)  #注意这里对索引进行重新排序，否则后面合并的时候会出现错位\n",
    "\n",
    "train_label = pd.DataFrame(train_df[\"class\"].map({\"是\":1,\"否\":0})).reset_index(drop=True) \n",
    "\n",
    "#再跟原来的特征变量合并到一起\n",
    "train_concat = pd.concat([train_cate_ohe,train_continuous,train_label],axis=1)\n",
    "# train_concat[:2]\n",
    "len(train_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用训练数据来建立一棵决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 17)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_concat),len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree_algorithm(train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sugar <= 0.20650000000000002': [0,\n",
      "                                  {'density <= 0.38149999999999995': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "照例先对测试数据也进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先对测试数据也进行一些数据预处理，注意要直接使用那些用于处理训练数据的转换操作实例，可以直接调用在训练数据fit好的算子\n",
    "test_cate = test_df[[\"color\",\"root\",\"noise\",\"texture\",\"navel\",\"touch\"]]\n",
    "#ohe是上面已经在训练数据上fit后的算子，这里可以直接调用\n",
    "test_cate_ohe = pd.DataFrame(ohe.transform(test_cate).toarray())\n",
    "test_cate_ohe.columns = ohe.get_feature_names()\n",
    "\n",
    "test_continous = pd.DataFrame(test_df[[\"density\",\"sugar\"]]).reset_index(drop=True)\n",
    "\n",
    "#对标签变量进行0-1编码，对于二分类，可以使用LabelBinarizer，但笔者更喜欢直接用map来显示转换\n",
    "test_label = pd.DataFrame(test_df[\"class\"].map({\"是\":1,\"否\":0})).reset_index(drop=True) \n",
    "\n",
    "#合并\n",
    "test_concat = pd.concat([test_cate_ohe,test_continous,test_label],axis=1)\n",
    "len(test_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = test_concat.apply(classify_example, axis=1, args=(tree,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_real = test_concat[\"class\"]\n",
    "accuracy_score(y_predict,y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们基本实现了一个无剪枝逻辑的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
